{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of ImageClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57EOxN_XNHG"
      },
      "source": [
        "## Image Classification with Convolutional Neural Networks\n",
        "We will revisit CNNs this time on CIFAR10, which has 60000 32x32 images from classes: aeroplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srq6HHoWXNHM"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras import backend as K\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC4PO2EwXNHN"
      },
      "source": [
        "A lot of the initialisation will be similar to what we have seen in the last practical. Lets start by defining the number of classes and some hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wtc02DHXNHN"
      },
      "source": [
        "num_classes = 10 #Cifar10 has 10 classes\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 32, 32"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMtiD0ESXNHO",
        "outputId": "1b890b2e-cbf6-4b9d-fe3f-0ae86614970b"
      },
      "source": [
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qopmCtkKICKG"
      },
      "source": [
        "As we did last time, lets normalise the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgqez_KEXNHP",
        "outputId": "8a1c0cfb-923b-4301-9d99-3be8e9464eef"
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255 # This converts the pixel values from between 0 and 255 to between 0 and 1\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfKUrCrLXNHQ"
      },
      "source": [
        "As we did last practical, we want to set up y_train and y_test to have a vector of size 10 for each input sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fsGBy6hXNHQ"
      },
      "source": [
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDZshbP9XNHQ",
        "outputId": "cf6d537d-77c6-4dab-988c-97e130a6c0df"
      },
      "source": [
        "y_train.shape # Make sure this is (50000,10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVSJv-eeXNHQ"
      },
      "source": [
        "### Building the model\n",
        "We are going to build a model that combines everything we have looked at in the Lecture 5 - see the video \"Putting it all together\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5wf2TBLXNHR",
        "outputId": "e9738370-2049-49cb-f5f2-b37645b1d082"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(16, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=(32,32,3), padding='same')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Flatten()) # This line is to convert from matrices to vectors\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(10, activation='softmax')) \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 16)        448       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 32, 32, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 32, 32, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 16, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 16, 16, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 8, 8, 32)          9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 8, 8, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 4, 4, 32)          9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 4, 4, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 90,986\n",
            "Trainable params: 90,762\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEpddAzaXNHR"
      },
      "source": [
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIbba6vuXNHR"
      },
      "source": [
        "### Training the model\n",
        "**Make sure you are using a GPU!** Otherwise this will be very slow.\n",
        "\n",
        "It will take a little bit of time to train anyway so run the model, make a cup of tea, and watch some youtube (ideally my lectures!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9OW2SJ2XNHR",
        "outputId": "47e03c9b-4cbf-485c-e60e-6ff6a36e8aa2"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 2.4442 - accuracy: 0.1923 - val_loss: 2.1184 - val_accuracy: 0.2246\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.8142 - accuracy: 0.3305 - val_loss: 1.7173 - val_accuracy: 0.3602\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6706 - accuracy: 0.3827 - val_loss: 1.7714 - val_accuracy: 0.3645\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.5846 - accuracy: 0.4148 - val_loss: 1.8023 - val_accuracy: 0.3600\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.5320 - accuracy: 0.4365 - val_loss: 1.9201 - val_accuracy: 0.3585\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.4735 - accuracy: 0.4535 - val_loss: 1.5857 - val_accuracy: 0.4234\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.4385 - accuracy: 0.4701 - val_loss: 1.6796 - val_accuracy: 0.4315\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.3969 - accuracy: 0.4876 - val_loss: 1.4499 - val_accuracy: 0.4757\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.3751 - accuracy: 0.5021 - val_loss: 1.6056 - val_accuracy: 0.4577\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.3449 - accuracy: 0.5083 - val_loss: 1.8568 - val_accuracy: 0.3834\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.3223 - accuracy: 0.5183 - val_loss: 1.5184 - val_accuracy: 0.4871\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.2918 - accuracy: 0.5307 - val_loss: 1.3903 - val_accuracy: 0.4938\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.2728 - accuracy: 0.5423 - val_loss: 1.4141 - val_accuracy: 0.4886\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.2375 - accuracy: 0.5511 - val_loss: 1.7385 - val_accuracy: 0.4296\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.2273 - accuracy: 0.5577 - val_loss: 1.2373 - val_accuracy: 0.5506\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.2138 - accuracy: 0.5634 - val_loss: 1.3181 - val_accuracy: 0.5367\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.1930 - accuracy: 0.5676 - val_loss: 1.5350 - val_accuracy: 0.4731\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.1780 - accuracy: 0.5793 - val_loss: 1.2273 - val_accuracy: 0.5554\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.1593 - accuracy: 0.5820 - val_loss: 1.3282 - val_accuracy: 0.5304\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.1340 - accuracy: 0.5921 - val_loss: 1.3297 - val_accuracy: 0.5379\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.1314 - accuracy: 0.5930 - val_loss: 1.3398 - val_accuracy: 0.5352\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.1179 - accuracy: 0.5989 - val_loss: 1.3094 - val_accuracy: 0.5427\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.1048 - accuracy: 0.6040 - val_loss: 1.2443 - val_accuracy: 0.5651\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.0924 - accuracy: 0.6088 - val_loss: 1.1561 - val_accuracy: 0.5820\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.0765 - accuracy: 0.6168 - val_loss: 1.3379 - val_accuracy: 0.5281\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.0749 - accuracy: 0.6148 - val_loss: 1.2460 - val_accuracy: 0.5523\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.0630 - accuracy: 0.6183 - val_loss: 1.1759 - val_accuracy: 0.5837\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.0459 - accuracy: 0.6251 - val_loss: 1.2755 - val_accuracy: 0.5476\n",
            "Epoch 29/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.0374 - accuracy: 0.6296 - val_loss: 1.6390 - val_accuracy: 0.4814\n",
            "Epoch 30/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.0204 - accuracy: 0.6385 - val_loss: 1.4132 - val_accuracy: 0.5206\n",
            "Epoch 31/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.0221 - accuracy: 0.6343 - val_loss: 1.2401 - val_accuracy: 0.5856\n",
            "Epoch 32/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.0192 - accuracy: 0.6373 - val_loss: 1.6283 - val_accuracy: 0.4686\n",
            "Epoch 33/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9989 - accuracy: 0.6421 - val_loss: 1.2155 - val_accuracy: 0.5770\n",
            "Epoch 34/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9838 - accuracy: 0.6476 - val_loss: 1.1187 - val_accuracy: 0.6063\n",
            "Epoch 35/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9933 - accuracy: 0.6445 - val_loss: 1.7756 - val_accuracy: 0.4459\n",
            "Epoch 36/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9739 - accuracy: 0.6514 - val_loss: 1.0392 - val_accuracy: 0.6295\n",
            "Epoch 37/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9659 - accuracy: 0.6575 - val_loss: 1.3248 - val_accuracy: 0.5549\n",
            "Epoch 38/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9640 - accuracy: 0.6565 - val_loss: 1.0431 - val_accuracy: 0.6261\n",
            "Epoch 39/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9638 - accuracy: 0.6568 - val_loss: 1.1150 - val_accuracy: 0.5993\n",
            "Epoch 40/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9531 - accuracy: 0.6598 - val_loss: 1.2025 - val_accuracy: 0.5884\n",
            "Epoch 41/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9598 - accuracy: 0.6599 - val_loss: 1.6693 - val_accuracy: 0.4845\n",
            "Epoch 42/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.9448 - accuracy: 0.6628 - val_loss: 1.1494 - val_accuracy: 0.6003\n",
            "Epoch 43/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9326 - accuracy: 0.6673 - val_loss: 1.1641 - val_accuracy: 0.5917\n",
            "Epoch 44/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.9348 - accuracy: 0.6693 - val_loss: 1.0546 - val_accuracy: 0.6344\n",
            "Epoch 45/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9229 - accuracy: 0.6699 - val_loss: 1.0908 - val_accuracy: 0.6206\n",
            "Epoch 46/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9228 - accuracy: 0.6732 - val_loss: 1.4066 - val_accuracy: 0.5444\n",
            "Epoch 47/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9146 - accuracy: 0.6778 - val_loss: 1.1829 - val_accuracy: 0.5950\n",
            "Epoch 48/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9068 - accuracy: 0.6789 - val_loss: 1.1899 - val_accuracy: 0.5908\n",
            "Epoch 49/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9023 - accuracy: 0.6791 - val_loss: 1.1328 - val_accuracy: 0.6069\n",
            "Epoch 50/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8967 - accuracy: 0.6799 - val_loss: 1.3520 - val_accuracy: 0.5379\n",
            "Epoch 51/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8833 - accuracy: 0.6880 - val_loss: 1.4673 - val_accuracy: 0.5337\n",
            "Epoch 52/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8887 - accuracy: 0.6864 - val_loss: 0.9375 - val_accuracy: 0.6741\n",
            "Epoch 53/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8845 - accuracy: 0.6857 - val_loss: 1.1967 - val_accuracy: 0.5825\n",
            "Epoch 54/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8817 - accuracy: 0.6887 - val_loss: 1.1272 - val_accuracy: 0.6002\n",
            "Epoch 55/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8894 - accuracy: 0.6879 - val_loss: 1.5609 - val_accuracy: 0.5144\n",
            "Epoch 56/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8799 - accuracy: 0.6910 - val_loss: 0.9235 - val_accuracy: 0.6771\n",
            "Epoch 57/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8723 - accuracy: 0.6905 - val_loss: 0.9334 - val_accuracy: 0.6727\n",
            "Epoch 58/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8646 - accuracy: 0.6961 - val_loss: 1.4439 - val_accuracy: 0.5376\n",
            "Epoch 59/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8609 - accuracy: 0.6942 - val_loss: 1.4992 - val_accuracy: 0.5161\n",
            "Epoch 60/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8546 - accuracy: 0.6995 - val_loss: 1.6729 - val_accuracy: 0.4790\n",
            "Epoch 61/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8534 - accuracy: 0.6973 - val_loss: 1.2279 - val_accuracy: 0.5860\n",
            "Epoch 62/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8567 - accuracy: 0.6983 - val_loss: 1.3858 - val_accuracy: 0.5479\n",
            "Epoch 63/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8362 - accuracy: 0.7036 - val_loss: 1.3452 - val_accuracy: 0.5518\n",
            "Epoch 64/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8401 - accuracy: 0.7052 - val_loss: 0.9878 - val_accuracy: 0.6538\n",
            "Epoch 65/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8399 - accuracy: 0.7044 - val_loss: 0.9620 - val_accuracy: 0.6636\n",
            "Epoch 66/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8393 - accuracy: 0.7026 - val_loss: 1.2619 - val_accuracy: 0.6017\n",
            "Epoch 67/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8271 - accuracy: 0.7085 - val_loss: 1.3821 - val_accuracy: 0.5524\n",
            "Epoch 68/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8243 - accuracy: 0.7100 - val_loss: 0.8758 - val_accuracy: 0.6886\n",
            "Epoch 69/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8254 - accuracy: 0.7094 - val_loss: 1.3608 - val_accuracy: 0.5574\n",
            "Epoch 70/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8230 - accuracy: 0.7098 - val_loss: 1.8292 - val_accuracy: 0.4591\n",
            "Epoch 71/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8161 - accuracy: 0.7128 - val_loss: 1.3494 - val_accuracy: 0.5598\n",
            "Epoch 72/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8154 - accuracy: 0.7140 - val_loss: 1.1578 - val_accuracy: 0.6110\n",
            "Epoch 73/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8180 - accuracy: 0.7144 - val_loss: 1.1136 - val_accuracy: 0.6195\n",
            "Epoch 74/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8041 - accuracy: 0.7150 - val_loss: 2.1630 - val_accuracy: 0.4073\n",
            "Epoch 75/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8072 - accuracy: 0.7160 - val_loss: 1.0278 - val_accuracy: 0.6482\n",
            "Epoch 76/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8012 - accuracy: 0.7168 - val_loss: 1.0910 - val_accuracy: 0.6275\n",
            "Epoch 77/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.8052 - accuracy: 0.7160 - val_loss: 1.2064 - val_accuracy: 0.5992\n",
            "Epoch 78/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7945 - accuracy: 0.7235 - val_loss: 1.2179 - val_accuracy: 0.5916\n",
            "Epoch 79/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7839 - accuracy: 0.7241 - val_loss: 1.0707 - val_accuracy: 0.6349\n",
            "Epoch 80/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.8001 - accuracy: 0.7183 - val_loss: 0.9289 - val_accuracy: 0.6757\n",
            "Epoch 81/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7922 - accuracy: 0.7224 - val_loss: 0.8147 - val_accuracy: 0.7134\n",
            "Epoch 82/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7898 - accuracy: 0.7215 - val_loss: 1.2396 - val_accuracy: 0.5923\n",
            "Epoch 83/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7859 - accuracy: 0.7246 - val_loss: 1.0289 - val_accuracy: 0.6471\n",
            "Epoch 84/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7832 - accuracy: 0.7232 - val_loss: 1.0641 - val_accuracy: 0.6355\n",
            "Epoch 85/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7778 - accuracy: 0.7239 - val_loss: 1.2055 - val_accuracy: 0.5966\n",
            "Epoch 86/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.7838 - accuracy: 0.7237 - val_loss: 0.9643 - val_accuracy: 0.6692\n",
            "Epoch 87/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.7745 - accuracy: 0.7267 - val_loss: 1.6272 - val_accuracy: 0.5036\n",
            "Epoch 88/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7890 - accuracy: 0.7212 - val_loss: 1.0171 - val_accuracy: 0.6392\n",
            "Epoch 89/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.7699 - accuracy: 0.7295 - val_loss: 0.8516 - val_accuracy: 0.7014\n",
            "Epoch 90/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7757 - accuracy: 0.7270 - val_loss: 0.8522 - val_accuracy: 0.6985\n",
            "Epoch 91/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7692 - accuracy: 0.7310 - val_loss: 1.3878 - val_accuracy: 0.5646\n",
            "Epoch 92/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.7660 - accuracy: 0.7301 - val_loss: 1.2201 - val_accuracy: 0.6032\n",
            "Epoch 93/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7671 - accuracy: 0.7302 - val_loss: 0.9053 - val_accuracy: 0.6820\n",
            "Epoch 94/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7676 - accuracy: 0.7267 - val_loss: 0.8990 - val_accuracy: 0.6834\n",
            "Epoch 95/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7639 - accuracy: 0.7324 - val_loss: 0.8548 - val_accuracy: 0.6890\n",
            "Epoch 96/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7527 - accuracy: 0.7351 - val_loss: 1.6697 - val_accuracy: 0.5033\n",
            "Epoch 97/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7617 - accuracy: 0.7329 - val_loss: 1.7254 - val_accuracy: 0.4914\n",
            "Epoch 98/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7542 - accuracy: 0.7339 - val_loss: 1.2124 - val_accuracy: 0.6084\n",
            "Epoch 99/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7569 - accuracy: 0.7338 - val_loss: 1.0694 - val_accuracy: 0.6403\n",
            "Epoch 100/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.7551 - accuracy: 0.7354 - val_loss: 0.9834 - val_accuracy: 0.6590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrAATEG2LF9i"
      },
      "source": [
        "The loss is still not as close to 0 as we would like, but we can see that it's still decreasing each epoch. This means that we should train it for longer to get maximum performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JhJGwZGXNHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fce7687-6139-4bad-cded-f1a7df4b37ee"
      },
      "source": [
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.9833546876907349\n",
            "Test accuracy: 0.6589999794960022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd7hBGuMLqsb"
      },
      "source": [
        "We get 66% performance which isn't great but also isn't too bad (random guessing would be 10%). Now lets trying building a deep model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4L-Ufi4MA-6",
        "outputId": "5a404365-460b-4e34-a337-5a16ca51080c"
      },
      "source": [
        "deep_model = Sequential()\r\n",
        "deep_model.add(Conv2D(32, kernel_size=(3, 3),\r\n",
        "                 activation='relu',\r\n",
        "                 input_shape=(32,32,3), padding='same')) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(32, kernel_size=(3, 3),\r\n",
        "                 activation='relu', padding='same')) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(32, kernel_size=(3, 3),\r\n",
        "                 activation='relu', padding='same')) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(32, kernel_size=(3, 3),\r\n",
        "                 activation='relu', padding='same')) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(64, kernel_size=(3, 3),\r\n",
        "                 activation='relu', padding='same', strides=2)) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3)) \r\n",
        "deep_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Flatten()) # This line is to convert from matrices to vectors\r\n",
        "deep_model.add(Dense(128, activation='relu'))\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Dense(10, activation='softmax')) \r\n",
        "deep_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_70 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_71 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_80 (Dropout)         (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_81 (Dropout)         (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_73 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_82 (Dropout)         (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_74 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_83 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_75 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_84 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_76 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_85 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_77 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_86 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_78 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_87 (Dropout)         (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_79 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_88 (Dropout)         (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_80 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_89 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_90 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 793,578\n",
            "Trainable params: 792,042\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuuIUc3RNMpx"
      },
      "source": [
        "deep_model.compile(loss=categorical_crossentropy,\r\n",
        "              optimizer='sgd',\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3rO4PUlNPaV",
        "outputId": "58181283-9764-492e-927f-4a0df8183806"
      },
      "source": [
        "deep_model.fit(x_train, y_train,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=epochs,\r\n",
        "          verbose=1,\r\n",
        "          validation_data=(x_test, y_test))\r\n",
        "score = deep_model.evaluate(x_test, y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 15s 34ms/step - loss: 2.8149 - accuracy: 0.1256 - val_loss: 2.5942 - val_accuracy: 0.1865\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.9413 - accuracy: 0.2800 - val_loss: 2.1596 - val_accuracy: 0.2665\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.8015 - accuracy: 0.3278 - val_loss: 2.0895 - val_accuracy: 0.2827\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.7138 - accuracy: 0.3616 - val_loss: 2.1830 - val_accuracy: 0.2851\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.6489 - accuracy: 0.3878 - val_loss: 1.8909 - val_accuracy: 0.3450\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.5934 - accuracy: 0.4136 - val_loss: 1.7971 - val_accuracy: 0.3784\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.5322 - accuracy: 0.4362 - val_loss: 1.9142 - val_accuracy: 0.3738\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.4931 - accuracy: 0.4542 - val_loss: 1.6005 - val_accuracy: 0.4391\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.4651 - accuracy: 0.4625 - val_loss: 1.5958 - val_accuracy: 0.4380\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.4297 - accuracy: 0.4777 - val_loss: 1.4073 - val_accuracy: 0.4950\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.3859 - accuracy: 0.4912 - val_loss: 1.5088 - val_accuracy: 0.4773\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.3569 - accuracy: 0.5067 - val_loss: 1.5949 - val_accuracy: 0.4510\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.3298 - accuracy: 0.5145 - val_loss: 1.4035 - val_accuracy: 0.5041\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.3055 - accuracy: 0.5284 - val_loss: 1.3348 - val_accuracy: 0.5259\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.2810 - accuracy: 0.5380 - val_loss: 1.2823 - val_accuracy: 0.5381\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.2567 - accuracy: 0.5443 - val_loss: 1.3518 - val_accuracy: 0.5224\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.2354 - accuracy: 0.5548 - val_loss: 1.1606 - val_accuracy: 0.5814\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.2177 - accuracy: 0.5584 - val_loss: 1.3717 - val_accuracy: 0.5236\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.2096 - accuracy: 0.5627 - val_loss: 1.1943 - val_accuracy: 0.5750\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.1870 - accuracy: 0.5691 - val_loss: 1.3961 - val_accuracy: 0.5284\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.1704 - accuracy: 0.5742 - val_loss: 1.1960 - val_accuracy: 0.5762\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 1.1720 - accuracy: 0.5787 - val_loss: 1.2557 - val_accuracy: 0.5684\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.1531 - accuracy: 0.5859 - val_loss: 1.1802 - val_accuracy: 0.5782\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.1338 - accuracy: 0.5942 - val_loss: 1.1397 - val_accuracy: 0.5979\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.1159 - accuracy: 0.6003 - val_loss: 1.1514 - val_accuracy: 0.5950\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.1045 - accuracy: 0.6027 - val_loss: 1.1451 - val_accuracy: 0.6009\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.0990 - accuracy: 0.6047 - val_loss: 1.1915 - val_accuracy: 0.5836\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.0892 - accuracy: 0.6086 - val_loss: 1.1074 - val_accuracy: 0.6023\n",
            "Epoch 29/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.0695 - accuracy: 0.6155 - val_loss: 1.1117 - val_accuracy: 0.6044\n",
            "Epoch 30/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.0515 - accuracy: 0.6250 - val_loss: 1.0521 - val_accuracy: 0.6310\n",
            "Epoch 31/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.0549 - accuracy: 0.6208 - val_loss: 1.3348 - val_accuracy: 0.5474\n",
            "Epoch 32/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.0373 - accuracy: 0.6277 - val_loss: 1.3470 - val_accuracy: 0.5466\n",
            "Epoch 33/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 1.0242 - accuracy: 0.6325 - val_loss: 1.2008 - val_accuracy: 0.5811\n",
            "Epoch 34/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 1.0155 - accuracy: 0.6370 - val_loss: 1.1508 - val_accuracy: 0.5986\n",
            "Epoch 35/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 1.0062 - accuracy: 0.6379 - val_loss: 1.1832 - val_accuracy: 0.5807\n",
            "Epoch 36/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.9896 - accuracy: 0.6480 - val_loss: 1.0329 - val_accuracy: 0.6301\n",
            "Epoch 37/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.9830 - accuracy: 0.6485 - val_loss: 0.9739 - val_accuracy: 0.6573\n",
            "Epoch 38/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.9821 - accuracy: 0.6469 - val_loss: 0.9653 - val_accuracy: 0.6540\n",
            "Epoch 39/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.9777 - accuracy: 0.6464 - val_loss: 0.9616 - val_accuracy: 0.6546\n",
            "Epoch 40/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.9613 - accuracy: 0.6575 - val_loss: 1.0000 - val_accuracy: 0.6480\n",
            "Epoch 41/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.9564 - accuracy: 0.6560 - val_loss: 1.1856 - val_accuracy: 0.5866\n",
            "Epoch 42/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.9411 - accuracy: 0.6680 - val_loss: 0.9049 - val_accuracy: 0.6801\n",
            "Epoch 43/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.9382 - accuracy: 0.6659 - val_loss: 0.9138 - val_accuracy: 0.6701\n",
            "Epoch 44/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.9349 - accuracy: 0.6675 - val_loss: 0.9873 - val_accuracy: 0.6487\n",
            "Epoch 45/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.9288 - accuracy: 0.6693 - val_loss: 1.1348 - val_accuracy: 0.6055\n",
            "Epoch 46/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.9184 - accuracy: 0.6740 - val_loss: 0.8651 - val_accuracy: 0.6928\n",
            "Epoch 47/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.8991 - accuracy: 0.6781 - val_loss: 1.0078 - val_accuracy: 0.6463\n",
            "Epoch 48/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.8985 - accuracy: 0.6809 - val_loss: 1.0413 - val_accuracy: 0.6408\n",
            "Epoch 49/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.8962 - accuracy: 0.6780 - val_loss: 0.9231 - val_accuracy: 0.6735\n",
            "Epoch 50/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.8945 - accuracy: 0.6815 - val_loss: 0.8849 - val_accuracy: 0.6892\n",
            "Epoch 51/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.8900 - accuracy: 0.6847 - val_loss: 0.9488 - val_accuracy: 0.6644\n",
            "Epoch 52/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.8740 - accuracy: 0.6900 - val_loss: 0.8945 - val_accuracy: 0.6846\n",
            "Epoch 53/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.8658 - accuracy: 0.6912 - val_loss: 0.8852 - val_accuracy: 0.6877\n",
            "Epoch 54/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.8672 - accuracy: 0.6912 - val_loss: 0.8212 - val_accuracy: 0.7105\n",
            "Epoch 55/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.8465 - accuracy: 0.6992 - val_loss: 0.8396 - val_accuracy: 0.7035\n",
            "Epoch 56/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.8464 - accuracy: 0.6974 - val_loss: 0.8698 - val_accuracy: 0.6954\n",
            "Epoch 57/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.8493 - accuracy: 0.6950 - val_loss: 0.8222 - val_accuracy: 0.7097\n",
            "Epoch 58/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.8401 - accuracy: 0.6993 - val_loss: 0.9301 - val_accuracy: 0.6781\n",
            "Epoch 59/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.8336 - accuracy: 0.7023 - val_loss: 0.8107 - val_accuracy: 0.7162\n",
            "Epoch 60/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.8276 - accuracy: 0.7069 - val_loss: 0.8466 - val_accuracy: 0.7039\n",
            "Epoch 61/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.8098 - accuracy: 0.7149 - val_loss: 0.7949 - val_accuracy: 0.7215\n",
            "Epoch 62/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.8134 - accuracy: 0.7117 - val_loss: 0.8511 - val_accuracy: 0.7024\n",
            "Epoch 63/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.8086 - accuracy: 0.7118 - val_loss: 0.8165 - val_accuracy: 0.7138\n",
            "Epoch 64/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.8050 - accuracy: 0.7132 - val_loss: 0.7977 - val_accuracy: 0.7180\n",
            "Epoch 65/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.8069 - accuracy: 0.7134 - val_loss: 0.8428 - val_accuracy: 0.7064\n",
            "Epoch 66/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7972 - accuracy: 0.7188 - val_loss: 0.7521 - val_accuracy: 0.7367\n",
            "Epoch 67/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7989 - accuracy: 0.7165 - val_loss: 0.8479 - val_accuracy: 0.7037\n",
            "Epoch 68/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7965 - accuracy: 0.7179 - val_loss: 0.8775 - val_accuracy: 0.6921\n",
            "Epoch 69/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7913 - accuracy: 0.7172 - val_loss: 0.8372 - val_accuracy: 0.7078\n",
            "Epoch 70/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7742 - accuracy: 0.7239 - val_loss: 0.7439 - val_accuracy: 0.7373\n",
            "Epoch 71/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.7759 - accuracy: 0.7252 - val_loss: 0.7713 - val_accuracy: 0.7277\n",
            "Epoch 72/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.7657 - accuracy: 0.7273 - val_loss: 0.7895 - val_accuracy: 0.7219\n",
            "Epoch 73/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7626 - accuracy: 0.7283 - val_loss: 0.8644 - val_accuracy: 0.7015\n",
            "Epoch 74/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7704 - accuracy: 0.7257 - val_loss: 0.7330 - val_accuracy: 0.7439\n",
            "Epoch 75/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7560 - accuracy: 0.7324 - val_loss: 1.0283 - val_accuracy: 0.6563\n",
            "Epoch 76/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.7619 - accuracy: 0.7302 - val_loss: 0.7379 - val_accuracy: 0.7426\n",
            "Epoch 77/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.7502 - accuracy: 0.7301 - val_loss: 0.7481 - val_accuracy: 0.7379\n",
            "Epoch 78/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.7426 - accuracy: 0.7367 - val_loss: 0.8034 - val_accuracy: 0.7199\n",
            "Epoch 79/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.7350 - accuracy: 0.7365 - val_loss: 0.7730 - val_accuracy: 0.7330\n",
            "Epoch 80/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7357 - accuracy: 0.7389 - val_loss: 0.7423 - val_accuracy: 0.7463\n",
            "Epoch 81/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.7275 - accuracy: 0.7418 - val_loss: 1.0339 - val_accuracy: 0.6475\n",
            "Epoch 82/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7298 - accuracy: 0.7401 - val_loss: 0.7938 - val_accuracy: 0.7216\n",
            "Epoch 83/100\n",
            "391/391 [==============================] - 14s 35ms/step - loss: 0.7380 - accuracy: 0.7415 - val_loss: 0.7628 - val_accuracy: 0.7392\n",
            "Epoch 84/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7203 - accuracy: 0.7436 - val_loss: 0.7507 - val_accuracy: 0.7360\n",
            "Epoch 85/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7179 - accuracy: 0.7450 - val_loss: 0.6926 - val_accuracy: 0.7581\n",
            "Epoch 86/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7078 - accuracy: 0.7475 - val_loss: 0.7409 - val_accuracy: 0.7413\n",
            "Epoch 87/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7157 - accuracy: 0.7464 - val_loss: 0.7574 - val_accuracy: 0.7356\n",
            "Epoch 88/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.6975 - accuracy: 0.7528 - val_loss: 0.7365 - val_accuracy: 0.7454\n",
            "Epoch 89/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7017 - accuracy: 0.7520 - val_loss: 0.7608 - val_accuracy: 0.7347\n",
            "Epoch 90/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7000 - accuracy: 0.7504 - val_loss: 0.7254 - val_accuracy: 0.7429\n",
            "Epoch 91/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.7020 - accuracy: 0.7506 - val_loss: 0.7301 - val_accuracy: 0.7439\n",
            "Epoch 92/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.6884 - accuracy: 0.7558 - val_loss: 0.6861 - val_accuracy: 0.7635\n",
            "Epoch 93/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.6921 - accuracy: 0.7580 - val_loss: 0.7171 - val_accuracy: 0.7475\n",
            "Epoch 94/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.6796 - accuracy: 0.7578 - val_loss: 0.6957 - val_accuracy: 0.7558\n",
            "Epoch 95/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.6794 - accuracy: 0.7583 - val_loss: 0.6775 - val_accuracy: 0.7597\n",
            "Epoch 96/100\n",
            "391/391 [==============================] - 13s 35ms/step - loss: 0.6714 - accuracy: 0.7629 - val_loss: 0.7662 - val_accuracy: 0.7361\n",
            "Epoch 97/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.6770 - accuracy: 0.7606 - val_loss: 0.7323 - val_accuracy: 0.7437\n",
            "Epoch 98/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.6681 - accuracy: 0.7608 - val_loss: 0.6329 - val_accuracy: 0.7774\n",
            "Epoch 99/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.6619 - accuracy: 0.7656 - val_loss: 0.6726 - val_accuracy: 0.7649\n",
            "Epoch 100/100\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 0.6607 - accuracy: 0.7654 - val_loss: 0.6878 - val_accuracy: 0.7645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9g7I6f8RzLb",
        "outputId": "4fa8ea1b-e94f-42a6-a9b1-bf7754fa0557"
      },
      "source": [
        "print('Test loss:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.6878185272216797\n",
            "Test accuracy: 0.7645000219345093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLgI2CMDROy5"
      },
      "source": [
        "If you have the patience to stick this out to the end (or you are rich enough to own 1000 TPUs to train on), you can see we get better results already. \r\n",
        "\r\n",
        "If you skipped ahead, my results were 65.90% accuracy for the shallow model, and 76.45% accuracy for the deep model. Note that neither model had fully converged to a loss of zero.\r\n",
        "\r\n",
        "### Exercise 1\r\n",
        "After the practical is over, try training both models for 5000 epochs (in the background while you are doing something else - don't sit and watch the model while it trains). Which model gets better performance? Have the models finished training? Do the models overfit?\r\n",
        "\r\n",
        "## Transfer Learning\r\n",
        "Lets finish this notebook off by exploring how to use other people's trained models and how to fine tune them on our data. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjJm2C3STtMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427b79b2-5fec-46b3-ed24-a73643e5d34a"
      },
      "source": [
        "from keras.applications import ResNet50, resnet50\r\n",
        "from keras.layers.experimental.preprocessing import Resizing\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "resnet = Sequential()   #We probably get better performance by resizing to 224,224 like the original resnet model, but that would be even slower. You can try this as an exercise\r\n",
        "# resnet.add( Resizing(\r\n",
        "#     224, 224, interpolation=\"bilinear\", name=None\r\n",
        "# ))\r\n",
        "resnet.add( ResNet50(\r\n",
        "    include_top=False,\r\n",
        "    weights=\"imagenet\", #this line allows us to load the pre-trained weights from ImageNet\r\n",
        "    input_tensor=None,\r\n",
        "    input_shape=(32,32,3),\r\n",
        "    pooling=None,\r\n",
        "    classes=10))\r\n",
        "    resnet.add(Flatten())\r\n",
        "    resnet.add(Dense(10, activation='softmax')) # Add this line to get down to 10 output classes"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkg-00I2tEOB"
      },
      "source": [
        "resnet.compile(loss=categorical_crossentropy,\r\n",
        "              optimizer='adam', #Note that I have used adam here instead of sgd. It is a little bit more advanced.\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYCrIh6vxyK"
      },
      "source": [
        "Finally, lets fit the model, stare at the screen, and pray that the loss goes down. Note this takes a really, really long time to train. You may want to go onto the next notebook and leave this one running in the background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmJFFCxVWUs8",
        "outputId": "528d22ef-f623-42a8-f7c7-a74c313aaf21"
      },
      "source": [
        "resnet.fit(x_train, y_train,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=epochs,\r\n",
        "          verbose=1,\r\n",
        "          validation_data=(x_test, y_test))\r\n",
        "score = resnet.evaluate(x_test, y_test, verbose=0)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 70s 167ms/step - loss: 1.2275 - accuracy: 0.5917 - val_loss: 22.3783 - val_accuracy: 0.1000\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 62s 160ms/step - loss: 0.9440 - accuracy: 0.6661 - val_loss: 10.4556 - val_accuracy: 0.1258\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.8600 - accuracy: 0.6943 - val_loss: 9.4332 - val_accuracy: 0.1998\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.7985 - accuracy: 0.7150 - val_loss: 6.5807 - val_accuracy: 0.2936\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.7475 - accuracy: 0.7312 - val_loss: 9.3317 - val_accuracy: 0.2156\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.7030 - accuracy: 0.7471 - val_loss: 3.8204 - val_accuracy: 0.4110\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.6599 - accuracy: 0.7639 - val_loss: 6.8507 - val_accuracy: 0.3049\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.6185 - accuracy: 0.7771 - val_loss: 7.3293 - val_accuracy: 0.2404\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.5799 - accuracy: 0.7908 - val_loss: 4.2548 - val_accuracy: 0.4677\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.5367 - accuracy: 0.8055 - val_loss: 5.3606 - val_accuracy: 0.4008\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.4983 - accuracy: 0.8222 - val_loss: 3.2274 - val_accuracy: 0.4640\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.4597 - accuracy: 0.8346 - val_loss: 3.9718 - val_accuracy: 0.4957\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.4207 - accuracy: 0.8484 - val_loss: 2.8036 - val_accuracy: 0.5356\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 62s 157ms/step - loss: 0.3872 - accuracy: 0.8593 - val_loss: 3.3457 - val_accuracy: 0.5101\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 62s 157ms/step - loss: 0.3496 - accuracy: 0.8742 - val_loss: 3.8331 - val_accuracy: 0.4682\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 62s 157ms/step - loss: 0.3214 - accuracy: 0.8829 - val_loss: 3.8406 - val_accuracy: 0.5265\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 62s 157ms/step - loss: 0.2855 - accuracy: 0.8975 - val_loss: 3.8724 - val_accuracy: 0.5263\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 61s 157ms/step - loss: 0.2569 - accuracy: 0.9063 - val_loss: 3.8333 - val_accuracy: 0.4849\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.2317 - accuracy: 0.9163 - val_loss: 2.5349 - val_accuracy: 0.5667\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 61s 157ms/step - loss: 0.2064 - accuracy: 0.9268 - val_loss: 2.5404 - val_accuracy: 0.5743\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 61s 157ms/step - loss: 0.1815 - accuracy: 0.9351 - val_loss: 2.3726 - val_accuracy: 0.6111\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 61s 157ms/step - loss: 0.1728 - accuracy: 0.9378 - val_loss: 3.0862 - val_accuracy: 0.5697\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 61s 157ms/step - loss: 0.1645 - accuracy: 0.9416 - val_loss: 3.2802 - val_accuracy: 0.5677\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 61s 157ms/step - loss: 0.1440 - accuracy: 0.9483 - val_loss: 3.6541 - val_accuracy: 0.5337\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 61s 157ms/step - loss: 0.1314 - accuracy: 0.9535 - val_loss: 3.2038 - val_accuracy: 0.5674\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 62s 157ms/step - loss: 0.1158 - accuracy: 0.9588 - val_loss: 3.0475 - val_accuracy: 0.5858\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 62s 157ms/step - loss: 0.1102 - accuracy: 0.9611 - val_loss: 3.2293 - val_accuracy: 0.5652\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.1160 - accuracy: 0.9579 - val_loss: 2.5825 - val_accuracy: 0.6293\n",
            "Epoch 29/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0956 - accuracy: 0.9664 - val_loss: 2.5075 - val_accuracy: 0.6285\n",
            "Epoch 30/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0913 - accuracy: 0.9672 - val_loss: 2.3596 - val_accuracy: 0.6430\n",
            "Epoch 31/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0895 - accuracy: 0.9685 - val_loss: 3.5359 - val_accuracy: 0.5441\n",
            "Epoch 32/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0955 - accuracy: 0.9670 - val_loss: 2.7279 - val_accuracy: 0.6219\n",
            "Epoch 33/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0763 - accuracy: 0.9734 - val_loss: 2.7581 - val_accuracy: 0.6109\n",
            "Epoch 34/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0744 - accuracy: 0.9743 - val_loss: 2.7419 - val_accuracy: 0.6410\n",
            "Epoch 35/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0738 - accuracy: 0.9736 - val_loss: 1.9917 - val_accuracy: 0.6896\n",
            "Epoch 36/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0769 - accuracy: 0.9729 - val_loss: 2.5629 - val_accuracy: 0.6291\n",
            "Epoch 37/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0712 - accuracy: 0.9752 - val_loss: 2.2273 - val_accuracy: 0.6720\n",
            "Epoch 38/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0610 - accuracy: 0.9792 - val_loss: 2.2773 - val_accuracy: 0.6762\n",
            "Epoch 39/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0615 - accuracy: 0.9789 - val_loss: 2.9846 - val_accuracy: 0.6159\n",
            "Epoch 40/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0704 - accuracy: 0.9758 - val_loss: 2.3130 - val_accuracy: 0.6605\n",
            "Epoch 41/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0584 - accuracy: 0.9795 - val_loss: 2.4976 - val_accuracy: 0.6725\n",
            "Epoch 42/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0575 - accuracy: 0.9800 - val_loss: 2.0605 - val_accuracy: 0.6849\n",
            "Epoch 43/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0625 - accuracy: 0.9782 - val_loss: 2.2696 - val_accuracy: 0.6663\n",
            "Epoch 44/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0629 - accuracy: 0.9787 - val_loss: 2.4629 - val_accuracy: 0.6475\n",
            "Epoch 45/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0532 - accuracy: 0.9818 - val_loss: 2.1011 - val_accuracy: 0.6784\n",
            "Epoch 46/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0508 - accuracy: 0.9818 - val_loss: 2.5619 - val_accuracy: 0.6513\n",
            "Epoch 47/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0465 - accuracy: 0.9844 - val_loss: 2.4743 - val_accuracy: 0.6558\n",
            "Epoch 48/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0633 - accuracy: 0.9779 - val_loss: 2.5615 - val_accuracy: 0.6450\n",
            "Epoch 49/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0543 - accuracy: 0.9814 - val_loss: 2.4556 - val_accuracy: 0.6673\n",
            "Epoch 50/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0500 - accuracy: 0.9825 - val_loss: 2.4245 - val_accuracy: 0.6677\n",
            "Epoch 51/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0471 - accuracy: 0.9840 - val_loss: 2.5733 - val_accuracy: 0.6778\n",
            "Epoch 52/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0460 - accuracy: 0.9843 - val_loss: 2.7656 - val_accuracy: 0.6800\n",
            "Epoch 53/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0436 - accuracy: 0.9850 - val_loss: 2.3634 - val_accuracy: 0.6820\n",
            "Epoch 54/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0500 - accuracy: 0.9832 - val_loss: 2.4148 - val_accuracy: 0.6589\n",
            "Epoch 55/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0475 - accuracy: 0.9845 - val_loss: 2.6425 - val_accuracy: 0.6604\n",
            "Epoch 56/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0403 - accuracy: 0.9860 - val_loss: 2.6907 - val_accuracy: 0.6637\n",
            "Epoch 57/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0458 - accuracy: 0.9839 - val_loss: 2.2410 - val_accuracy: 0.6927\n",
            "Epoch 58/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0461 - accuracy: 0.9845 - val_loss: 2.6110 - val_accuracy: 0.6556\n",
            "Epoch 59/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0537 - accuracy: 0.9810 - val_loss: 2.8500 - val_accuracy: 0.6622\n",
            "Epoch 60/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0378 - accuracy: 0.9863 - val_loss: 2.3245 - val_accuracy: 0.6924\n",
            "Epoch 61/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0428 - accuracy: 0.9850 - val_loss: 3.1401 - val_accuracy: 0.6161\n",
            "Epoch 62/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0399 - accuracy: 0.9858 - val_loss: 2.8430 - val_accuracy: 0.6488\n",
            "Epoch 63/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0404 - accuracy: 0.9862 - val_loss: 2.6037 - val_accuracy: 0.6555\n",
            "Epoch 64/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0386 - accuracy: 0.9868 - val_loss: 2.2095 - val_accuracy: 0.7107\n",
            "Epoch 65/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0400 - accuracy: 0.9857 - val_loss: 3.0483 - val_accuracy: 0.6268\n",
            "Epoch 66/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0359 - accuracy: 0.9881 - val_loss: 2.6269 - val_accuracy: 0.6824\n",
            "Epoch 67/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0355 - accuracy: 0.9887 - val_loss: 2.2238 - val_accuracy: 0.6960\n",
            "Epoch 68/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0331 - accuracy: 0.9886 - val_loss: 2.6202 - val_accuracy: 0.6863\n",
            "Epoch 69/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0392 - accuracy: 0.9866 - val_loss: 3.0652 - val_accuracy: 0.6368\n",
            "Epoch 70/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0327 - accuracy: 0.9892 - val_loss: 2.1716 - val_accuracy: 0.7130\n",
            "Epoch 71/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0555 - accuracy: 0.9812 - val_loss: 3.2480 - val_accuracy: 0.6369\n",
            "Epoch 72/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0380 - accuracy: 0.9868 - val_loss: 2.2875 - val_accuracy: 0.6936\n",
            "Epoch 73/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0248 - accuracy: 0.9919 - val_loss: 2.0315 - val_accuracy: 0.7223\n",
            "Epoch 74/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0284 - accuracy: 0.9907 - val_loss: 2.7470 - val_accuracy: 0.6852\n",
            "Epoch 75/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0403 - accuracy: 0.9862 - val_loss: 2.7931 - val_accuracy: 0.6456\n",
            "Epoch 76/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0411 - accuracy: 0.9860 - val_loss: 2.2307 - val_accuracy: 0.6929\n",
            "Epoch 77/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0350 - accuracy: 0.9879 - val_loss: 2.2835 - val_accuracy: 0.7008\n",
            "Epoch 78/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0311 - accuracy: 0.9901 - val_loss: 2.6372 - val_accuracy: 0.6565\n",
            "Epoch 79/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0298 - accuracy: 0.9899 - val_loss: 2.5625 - val_accuracy: 0.6682\n",
            "Epoch 80/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0367 - accuracy: 0.9876 - val_loss: 2.2611 - val_accuracy: 0.6950\n",
            "Epoch 81/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0301 - accuracy: 0.9896 - val_loss: 2.7274 - val_accuracy: 0.6772\n",
            "Epoch 82/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0377 - accuracy: 0.9873 - val_loss: 2.2714 - val_accuracy: 0.7071\n",
            "Epoch 83/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0272 - accuracy: 0.9911 - val_loss: 2.2609 - val_accuracy: 0.7118\n",
            "Epoch 84/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0278 - accuracy: 0.9906 - val_loss: 2.3632 - val_accuracy: 0.7042\n",
            "Epoch 85/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0395 - accuracy: 0.9868 - val_loss: 2.4749 - val_accuracy: 0.6766\n",
            "Epoch 86/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0365 - accuracy: 0.9876 - val_loss: 2.7897 - val_accuracy: 0.6692\n",
            "Epoch 87/100\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.0267 - accuracy: 0.9907 - val_loss: 2.7317 - val_accuracy: 0.6767\n",
            "Epoch 88/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0236 - accuracy: 0.9919 - val_loss: 2.2916 - val_accuracy: 0.6968\n",
            "Epoch 89/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0319 - accuracy: 0.9891 - val_loss: 2.6428 - val_accuracy: 0.6822\n",
            "Epoch 90/100\n",
            "391/391 [==============================] - 62s 157ms/step - loss: 0.0310 - accuracy: 0.9888 - val_loss: 2.3683 - val_accuracy: 0.6905\n",
            "Epoch 91/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0272 - accuracy: 0.9908 - val_loss: 2.5016 - val_accuracy: 0.6868\n",
            "Epoch 92/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0267 - accuracy: 0.9905 - val_loss: 2.2714 - val_accuracy: 0.7049\n",
            "Epoch 93/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 2.6037 - val_accuracy: 0.6812\n",
            "Epoch 94/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0271 - accuracy: 0.9908 - val_loss: 2.2439 - val_accuracy: 0.7121\n",
            "Epoch 95/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0408 - accuracy: 0.9862 - val_loss: 2.2741 - val_accuracy: 0.6865\n",
            "Epoch 96/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0288 - accuracy: 0.9902 - val_loss: 2.3135 - val_accuracy: 0.7042\n",
            "Epoch 97/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0234 - accuracy: 0.9920 - val_loss: 2.7089 - val_accuracy: 0.6836\n",
            "Epoch 98/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0284 - accuracy: 0.9905 - val_loss: 2.3141 - val_accuracy: 0.7189\n",
            "Epoch 99/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0243 - accuracy: 0.9919 - val_loss: 2.5446 - val_accuracy: 0.6887\n",
            "Epoch 100/100\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.0511 - accuracy: 0.9840 - val_loss: 3.8079 - val_accuracy: 0.6228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_mbNddFuPqN"
      },
      "source": [
        "As before, because this takes some time to train, you may need to try these exercises after the practical has ended.\r\n",
        "\r\n",
        "### Exercise 2\r\n",
        "Copy and paste the above output into a Word Document.\r\n",
        "\r\n",
        "Change resnet so it doesn't use pretrained weights from ImageNet - i.e. train it from scratch. How does your performance compare? Have a look at the train accuracy and val accuracy at each epoch, particularly in the first few epochs. What is happening here? Does it make sense?\r\n",
        "\r\n",
        "\r\n",
        "### Exercise 3\r\n",
        "The output of ResNet is a 2048 dimensional vector. This means we go from 2048 dimensions straight down to 10. We might be better off with an additional layer to decrease the dimensions a bit more gently. \r\n",
        "\r\n",
        "Add a new layer into the `resnet` model after Flatten that reduces the dimensionality to 256, before finally reducing the 10-dimensional output. \r\n",
        "\r\n",
        "(You can do this exercise with or without the ImageNet weights. Your choice\r\n",
        "\r\n",
        "### Exercise 4 (Advanced) \r\n",
        "Start your model off with ImageNet weights and your 256-dimensional hidden layer. Freeze the ImageNet weights and train only the last two layers for the first five epochs.\r\n",
        "\r\n",
        "### Exercise 5\r\n",
        "See how ResNet50 performs on the cifar100 data set. Try with and without ImageNet weights."
      ]
    }
  ]
}
