{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_vA97D7lrtH"
      },
      "source": [
        "# Data Augmentation\r\n",
        "\r\n",
        "In this notebook we will explore how to perform data augment data as shown in lecture 5. Its essential when using deep networks to get the most out of your data. Fortunately, performing keras makes augmenting data with its ImageDataGenerator\r\n",
        "\r\n",
        "(This first line is completely unnecessary - I changed my mind on what this notebook would contain. But, if you need to add new libraries in Google Colab, this is how you do it. I thought it might be a useful thing for you to know about so I've left it here.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWsM6oCnlqw4",
        "outputId": "a6db5f31-9878-475a-8219-5de6a46181e4"
      },
      "source": [
        "!pip install -q tfds-nightly tensorflow matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8MB 5.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-8JXk-4ZBol"
      },
      "source": [
        "Lets start with our imports, as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srq6HHoWXNHM"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMCGlPksauWu"
      },
      "source": [
        "This time we will load the fashion mnist data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzv9xP3siif2",
        "outputId": "7e97bc12-6fae-48eb-a265-df6a0913d172"
      },
      "source": [
        "batch_size = 32\r\n",
        "epochs = 100\r\n",
        "num_classes=10\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\r\n",
        "x_train.shape\r\n",
        "\r\n",
        "img_rows = 28\r\n",
        "img_cols = 28"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLumrDGtmt3l"
      },
      "source": [
        "Hopefully a lot of this code looks familiar to you by now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws-_nYnTmtDY",
        "outputId": "23b11d45-6ae0-47a8-82e4-ecb437b1d71d"
      },
      "source": [
        "x_train = x_train.astype('float32')\r\n",
        "x_test = x_test.astype('float32')\r\n",
        "x_train /= 255 \r\n",
        "x_test /= 255\r\n",
        "print('x_train shape:', x_train.shape)\r\n",
        "print(x_train.shape[0], 'train samples')\r\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9axA02HssfJ"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n",
        "input_shape = (img_rows, img_cols, 1)\r\n",
        "\r\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIVGoe1MoKfv"
      },
      "source": [
        "Lets also copy the model from the last notebook because it seemed to work quite well. (Note: how much nicer would it look if we just had a model.py file to store this code and then simply called deep_model from there. You can try this as an exercise if you wish)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4L-Ufi4MA-6",
        "outputId": "652fae3a-8cb1-4e29-a75d-4b77aa59b5a5"
      },
      "source": [
        "deep_model = Sequential()\r\n",
        "deep_model.add(Conv2D(32, kernel_size=(3, 3),\r\n",
        "                 activation='relu',\r\n",
        "                 input_shape=(28,28,1), padding='same')) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(32, kernel_size=(3, 3),\r\n",
        "                 activation='relu', padding='same')) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(32, kernel_size=(3, 3),\r\n",
        "                 activation='relu', padding='same')) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(32, kernel_size=(3, 3),\r\n",
        "                 activation='relu', padding='same')) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(64, kernel_size=(3, 3),\r\n",
        "                 activation='relu', padding='same', strides=2)) \r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3)) \r\n",
        "deep_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\r\n",
        "deep_model.add(BatchNormalization())\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Flatten()) # This line is to convert from matrices to vectors\r\n",
        "deep_model.add(Dense(128, activation='relu'))\r\n",
        "deep_model.add(Dropout(0.3))\r\n",
        "deep_model.add(Dense(10, activation='softmax')) \r\n",
        "deep_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 793,002\n",
            "Trainable params: 791,466\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXuZHE1eq_xG"
      },
      "source": [
        "Finally, lets compile the model again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkg-00I2tEOB"
      },
      "source": [
        "deep_model.compile(loss=categorical_crossentropy,\r\n",
        "              optimizer='adam', #Note that I have used adam here instead of sgd. It is a little bit more advanced.\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0A9A62lq1Av"
      },
      "source": [
        "Now we have the data loaded and the model compiled, let's explore how we can do data augmentation\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSM0ycVmo1Nt",
        "outputId": "2274a9a2-3dff-4e25-b409-93d1a2dfec86"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator    \r\n",
        "    \r\n",
        "datagen = ImageDataGenerator(\r\n",
        "    # randomly rotate images in the range (deg 0 to 180)\r\n",
        "    rotation_range=30,\r\n",
        "    # randomly shift images horizontally\r\n",
        "    width_shift_range=0.1,\r\n",
        "    # randomly shift images vertically\r\n",
        "    height_shift_range=0.1,\r\n",
        "    # set range for random zoom\r\n",
        "    zoom_range=0.5,\r\n",
        "    # set mode for filling points outside the input boundaries\r\n",
        "    fill_mode='nearest',\r\n",
        "    # We will usually flip images horizontally rather than vertically as these flips are more realistic in real life\r\n",
        "    horizontal_flip=True,\r\n",
        "    vertical_flip=False)\r\n",
        "\r\n",
        "datagen.fit(x_train)\r\n",
        "# datagen.flow() will apply your augmentations specified above to each image in the batch.\r\n",
        "deep_model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\r\n",
        "                    validation_data=(x_test, y_test),\r\n",
        "                    epochs=epochs, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1875/1875 [==============================] - 57s 14ms/step - loss: 1.7089 - accuracy: 0.3922 - val_loss: 0.7645 - val_accuracy: 0.6963\n",
            "Epoch 2/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.9932 - accuracy: 0.6324 - val_loss: 0.6845 - val_accuracy: 0.7341\n",
            "Epoch 3/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.8679 - accuracy: 0.6807 - val_loss: 0.5911 - val_accuracy: 0.7846\n",
            "Epoch 4/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.7879 - accuracy: 0.7137 - val_loss: 0.6122 - val_accuracy: 0.7670\n",
            "Epoch 5/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.7256 - accuracy: 0.7339 - val_loss: 0.6156 - val_accuracy: 0.7707\n",
            "Epoch 6/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6743 - accuracy: 0.7564 - val_loss: 0.5700 - val_accuracy: 0.7899\n",
            "Epoch 7/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6511 - accuracy: 0.7667 - val_loss: 0.5249 - val_accuracy: 0.8071\n",
            "Epoch 8/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.6228 - accuracy: 0.7758 - val_loss: 0.5526 - val_accuracy: 0.7991\n",
            "Epoch 9/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.6043 - accuracy: 0.7775 - val_loss: 0.5260 - val_accuracy: 0.8102\n",
            "Epoch 10/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5763 - accuracy: 0.7903 - val_loss: 0.4408 - val_accuracy: 0.8362\n",
            "Epoch 11/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5553 - accuracy: 0.8012 - val_loss: 0.4774 - val_accuracy: 0.8225\n",
            "Epoch 12/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5425 - accuracy: 0.8026 - val_loss: 0.4827 - val_accuracy: 0.8234\n",
            "Epoch 13/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5349 - accuracy: 0.8067 - val_loss: 0.4776 - val_accuracy: 0.8220\n",
            "Epoch 14/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5147 - accuracy: 0.8139 - val_loss: 0.4669 - val_accuracy: 0.8317\n",
            "Epoch 15/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5136 - accuracy: 0.8144 - val_loss: 0.6268 - val_accuracy: 0.7789\n",
            "Epoch 16/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5097 - accuracy: 0.8155 - val_loss: 0.4729 - val_accuracy: 0.8345\n",
            "Epoch 17/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4923 - accuracy: 0.8239 - val_loss: 0.4575 - val_accuracy: 0.8323\n",
            "Epoch 18/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4952 - accuracy: 0.8199 - val_loss: 0.5219 - val_accuracy: 0.8134\n",
            "Epoch 19/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4817 - accuracy: 0.8282 - val_loss: 0.4974 - val_accuracy: 0.8249\n",
            "Epoch 20/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4771 - accuracy: 0.8273 - val_loss: 0.4632 - val_accuracy: 0.8322\n",
            "Epoch 21/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4771 - accuracy: 0.8255 - val_loss: 0.4096 - val_accuracy: 0.8501\n",
            "Epoch 22/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4743 - accuracy: 0.8281 - val_loss: 0.5718 - val_accuracy: 0.8080\n",
            "Epoch 23/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4739 - accuracy: 0.8280 - val_loss: 0.3989 - val_accuracy: 0.8549\n",
            "Epoch 24/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4655 - accuracy: 0.8304 - val_loss: 0.5046 - val_accuracy: 0.8214\n",
            "Epoch 25/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4593 - accuracy: 0.8319 - val_loss: 0.5118 - val_accuracy: 0.8197\n",
            "Epoch 26/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4510 - accuracy: 0.8358 - val_loss: 0.4342 - val_accuracy: 0.8437\n",
            "Epoch 27/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4465 - accuracy: 0.8361 - val_loss: 0.4906 - val_accuracy: 0.8326\n",
            "Epoch 28/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4432 - accuracy: 0.8399 - val_loss: 0.5138 - val_accuracy: 0.8221\n",
            "Epoch 29/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4477 - accuracy: 0.8379 - val_loss: 0.4475 - val_accuracy: 0.8418\n",
            "Epoch 30/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4319 - accuracy: 0.8441 - val_loss: 0.4217 - val_accuracy: 0.8509\n",
            "Epoch 31/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4384 - accuracy: 0.8399 - val_loss: 0.4529 - val_accuracy: 0.8366\n",
            "Epoch 32/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4492 - accuracy: 0.8374 - val_loss: 0.5677 - val_accuracy: 0.8122\n",
            "Epoch 33/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4308 - accuracy: 0.8443 - val_loss: 0.4711 - val_accuracy: 0.8375\n",
            "Epoch 34/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4319 - accuracy: 0.8422 - val_loss: 0.4359 - val_accuracy: 0.8504\n",
            "Epoch 35/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4354 - accuracy: 0.8409 - val_loss: 0.4389 - val_accuracy: 0.8473\n",
            "Epoch 36/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.4302 - accuracy: 0.8426 - val_loss: 0.4686 - val_accuracy: 0.8336\n",
            "Epoch 37/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4321 - accuracy: 0.8442 - val_loss: 0.4289 - val_accuracy: 0.8469\n",
            "Epoch 38/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4301 - accuracy: 0.8442 - val_loss: 0.4371 - val_accuracy: 0.8441\n",
            "Epoch 39/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4237 - accuracy: 0.8473 - val_loss: 0.4814 - val_accuracy: 0.8377\n",
            "Epoch 40/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4319 - accuracy: 0.8434 - val_loss: 0.4513 - val_accuracy: 0.8412\n",
            "Epoch 41/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.4131 - accuracy: 0.8501 - val_loss: 0.5041 - val_accuracy: 0.8273\n",
            "Epoch 42/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.4169 - accuracy: 0.8476 - val_loss: 0.4783 - val_accuracy: 0.8361\n",
            "Epoch 43/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4187 - accuracy: 0.8486 - val_loss: 0.4728 - val_accuracy: 0.8364\n",
            "Epoch 44/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4156 - accuracy: 0.8488 - val_loss: 0.4131 - val_accuracy: 0.8496\n",
            "Epoch 45/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4089 - accuracy: 0.8514 - val_loss: 0.5232 - val_accuracy: 0.8249\n",
            "Epoch 46/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.4185 - accuracy: 0.8465 - val_loss: 0.4434 - val_accuracy: 0.8445\n",
            "Epoch 47/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4107 - accuracy: 0.8531 - val_loss: 0.3936 - val_accuracy: 0.8552\n",
            "Epoch 48/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.4056 - accuracy: 0.8506 - val_loss: 0.4618 - val_accuracy: 0.8442\n",
            "Epoch 49/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4098 - accuracy: 0.8503 - val_loss: 0.4691 - val_accuracy: 0.8422\n",
            "Epoch 50/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4067 - accuracy: 0.8545 - val_loss: 0.4564 - val_accuracy: 0.8438\n",
            "Epoch 51/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4132 - accuracy: 0.8514 - val_loss: 0.4684 - val_accuracy: 0.8381\n",
            "Epoch 52/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4060 - accuracy: 0.8526 - val_loss: 0.4164 - val_accuracy: 0.8494\n",
            "Epoch 53/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4065 - accuracy: 0.8502 - val_loss: 0.4499 - val_accuracy: 0.8486\n",
            "Epoch 54/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.4001 - accuracy: 0.8548 - val_loss: 0.4885 - val_accuracy: 0.8272\n",
            "Epoch 55/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4032 - accuracy: 0.8526 - val_loss: 0.3762 - val_accuracy: 0.8635\n",
            "Epoch 56/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4083 - accuracy: 0.8507 - val_loss: 0.3716 - val_accuracy: 0.8636\n",
            "Epoch 57/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4071 - accuracy: 0.8505 - val_loss: 0.4632 - val_accuracy: 0.8441\n",
            "Epoch 58/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3993 - accuracy: 0.8540 - val_loss: 0.4542 - val_accuracy: 0.8341\n",
            "Epoch 59/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3983 - accuracy: 0.8540 - val_loss: 0.4508 - val_accuracy: 0.8497\n",
            "Epoch 60/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3832 - accuracy: 0.8596 - val_loss: 0.4021 - val_accuracy: 0.8634\n",
            "Epoch 61/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3994 - accuracy: 0.8531 - val_loss: 0.4566 - val_accuracy: 0.8455\n",
            "Epoch 62/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3953 - accuracy: 0.8544 - val_loss: 0.4256 - val_accuracy: 0.8570\n",
            "Epoch 63/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3928 - accuracy: 0.8574 - val_loss: 0.4763 - val_accuracy: 0.8474\n",
            "Epoch 64/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3933 - accuracy: 0.8567 - val_loss: 0.4213 - val_accuracy: 0.8497\n",
            "Epoch 65/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.4019 - accuracy: 0.8545 - val_loss: 0.4024 - val_accuracy: 0.8685\n",
            "Epoch 66/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3857 - accuracy: 0.8613 - val_loss: 0.3992 - val_accuracy: 0.8601\n",
            "Epoch 67/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3955 - accuracy: 0.8565 - val_loss: 0.4712 - val_accuracy: 0.8473\n",
            "Epoch 68/150\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3932 - accuracy: 0.8576 - val_loss: 0.5109 - val_accuracy: 0.8311\n",
            "Epoch 69/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3898 - accuracy: 0.8592 - val_loss: 0.4494 - val_accuracy: 0.8421\n",
            "Epoch 70/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3926 - accuracy: 0.8581 - val_loss: 0.4209 - val_accuracy: 0.8573\n",
            "Epoch 71/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3899 - accuracy: 0.8582 - val_loss: 0.4476 - val_accuracy: 0.8547\n",
            "Epoch 72/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3925 - accuracy: 0.8566 - val_loss: 0.4798 - val_accuracy: 0.8435\n",
            "Epoch 73/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3888 - accuracy: 0.8586 - val_loss: 0.4534 - val_accuracy: 0.8507\n",
            "Epoch 74/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3910 - accuracy: 0.8558 - val_loss: 0.4329 - val_accuracy: 0.8511\n",
            "Epoch 75/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3895 - accuracy: 0.8578 - val_loss: 0.4575 - val_accuracy: 0.8451\n",
            "Epoch 76/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3851 - accuracy: 0.8610 - val_loss: 0.4590 - val_accuracy: 0.8454\n",
            "Epoch 77/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3798 - accuracy: 0.8625 - val_loss: 0.3910 - val_accuracy: 0.8630\n",
            "Epoch 78/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3776 - accuracy: 0.8613 - val_loss: 0.4412 - val_accuracy: 0.8532\n",
            "Epoch 79/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3854 - accuracy: 0.8606 - val_loss: 0.4164 - val_accuracy: 0.8591\n",
            "Epoch 80/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3828 - accuracy: 0.8622 - val_loss: 0.3959 - val_accuracy: 0.8629\n",
            "Epoch 81/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3787 - accuracy: 0.8623 - val_loss: 0.4010 - val_accuracy: 0.8611\n",
            "Epoch 82/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3829 - accuracy: 0.8611 - val_loss: 0.4252 - val_accuracy: 0.8609\n",
            "Epoch 83/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3853 - accuracy: 0.8610 - val_loss: 0.4535 - val_accuracy: 0.8484\n",
            "Epoch 84/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3838 - accuracy: 0.8591 - val_loss: 0.3687 - val_accuracy: 0.8676\n",
            "Epoch 85/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3725 - accuracy: 0.8631 - val_loss: 0.4301 - val_accuracy: 0.8522\n",
            "Epoch 86/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3781 - accuracy: 0.8643 - val_loss: 0.3581 - val_accuracy: 0.8710\n",
            "Epoch 87/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3758 - accuracy: 0.8649 - val_loss: 0.4006 - val_accuracy: 0.8593\n",
            "Epoch 88/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3765 - accuracy: 0.8624 - val_loss: 0.4175 - val_accuracy: 0.8578\n",
            "Epoch 89/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3720 - accuracy: 0.8637 - val_loss: 0.4171 - val_accuracy: 0.8562\n",
            "Epoch 90/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3714 - accuracy: 0.8628 - val_loss: 0.3956 - val_accuracy: 0.8615\n",
            "Epoch 91/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3810 - accuracy: 0.8624 - val_loss: 0.3872 - val_accuracy: 0.8663\n",
            "Epoch 92/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3695 - accuracy: 0.8641 - val_loss: 0.4733 - val_accuracy: 0.8389\n",
            "Epoch 93/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3795 - accuracy: 0.8622 - val_loss: 0.3881 - val_accuracy: 0.8640\n",
            "Epoch 94/150\n",
            "1875/1875 [==============================] - 25s 14ms/step - loss: 0.3802 - accuracy: 0.8620 - val_loss: 0.4346 - val_accuracy: 0.8572\n",
            "Epoch 95/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3697 - accuracy: 0.8643 - val_loss: 0.3932 - val_accuracy: 0.8604\n",
            "Epoch 96/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3776 - accuracy: 0.8613 - val_loss: 0.4984 - val_accuracy: 0.8447\n",
            "Epoch 97/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3782 - accuracy: 0.8639 - val_loss: 0.3650 - val_accuracy: 0.8712\n",
            "Epoch 98/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3675 - accuracy: 0.8668 - val_loss: 0.3854 - val_accuracy: 0.8670\n",
            "Epoch 99/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3738 - accuracy: 0.8641 - val_loss: 0.3636 - val_accuracy: 0.8703\n",
            "Epoch 100/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3682 - accuracy: 0.8653 - val_loss: 0.3860 - val_accuracy: 0.8670\n",
            "Epoch 101/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3621 - accuracy: 0.8687 - val_loss: 0.3932 - val_accuracy: 0.8691\n",
            "Epoch 102/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3641 - accuracy: 0.8680 - val_loss: 0.4072 - val_accuracy: 0.8645\n",
            "Epoch 103/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3710 - accuracy: 0.8648 - val_loss: 0.4043 - val_accuracy: 0.8611\n",
            "Epoch 104/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3709 - accuracy: 0.8641 - val_loss: 0.4518 - val_accuracy: 0.8484\n",
            "Epoch 105/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3721 - accuracy: 0.8652 - val_loss: 0.4567 - val_accuracy: 0.8514\n",
            "Epoch 106/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3732 - accuracy: 0.8641 - val_loss: 0.4047 - val_accuracy: 0.8612\n",
            "Epoch 107/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3702 - accuracy: 0.8644 - val_loss: 0.4872 - val_accuracy: 0.8407\n",
            "Epoch 108/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3705 - accuracy: 0.8658 - val_loss: 0.4364 - val_accuracy: 0.8568\n",
            "Epoch 109/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3676 - accuracy: 0.8640 - val_loss: 0.4422 - val_accuracy: 0.8468\n",
            "Epoch 110/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3680 - accuracy: 0.8673 - val_loss: 0.3971 - val_accuracy: 0.8635\n",
            "Epoch 111/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3714 - accuracy: 0.8657 - val_loss: 0.4635 - val_accuracy: 0.8539\n",
            "Epoch 112/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3630 - accuracy: 0.8682 - val_loss: 0.4058 - val_accuracy: 0.8635\n",
            "Epoch 113/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3605 - accuracy: 0.8669 - val_loss: 0.3770 - val_accuracy: 0.8662\n",
            "Epoch 114/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3649 - accuracy: 0.8691 - val_loss: 0.3898 - val_accuracy: 0.8662\n",
            "Epoch 115/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3630 - accuracy: 0.8680 - val_loss: 0.4256 - val_accuracy: 0.8589\n",
            "Epoch 116/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3649 - accuracy: 0.8665 - val_loss: 0.4015 - val_accuracy: 0.8532\n",
            "Epoch 117/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3699 - accuracy: 0.8658 - val_loss: 0.4769 - val_accuracy: 0.8469\n",
            "Epoch 118/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3563 - accuracy: 0.8688 - val_loss: 0.4193 - val_accuracy: 0.8586\n",
            "Epoch 119/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3632 - accuracy: 0.8676 - val_loss: 0.3847 - val_accuracy: 0.8670\n",
            "Epoch 120/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3600 - accuracy: 0.8684 - val_loss: 0.4112 - val_accuracy: 0.8624\n",
            "Epoch 121/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3588 - accuracy: 0.8713 - val_loss: 0.4251 - val_accuracy: 0.8656\n",
            "Epoch 122/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3590 - accuracy: 0.8679 - val_loss: 0.4133 - val_accuracy: 0.8613\n",
            "Epoch 123/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3659 - accuracy: 0.8676 - val_loss: 0.4108 - val_accuracy: 0.8624\n",
            "Epoch 124/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3640 - accuracy: 0.8673 - val_loss: 0.4342 - val_accuracy: 0.8531\n",
            "Epoch 125/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3661 - accuracy: 0.8680 - val_loss: 0.3719 - val_accuracy: 0.8689\n",
            "Epoch 126/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3569 - accuracy: 0.8692 - val_loss: 0.4068 - val_accuracy: 0.8628\n",
            "Epoch 127/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3620 - accuracy: 0.8699 - val_loss: 0.3747 - val_accuracy: 0.8704\n",
            "Epoch 128/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3595 - accuracy: 0.8694 - val_loss: 0.4536 - val_accuracy: 0.8475\n",
            "Epoch 129/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3595 - accuracy: 0.8700 - val_loss: 0.4344 - val_accuracy: 0.8565\n",
            "Epoch 130/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3591 - accuracy: 0.8679 - val_loss: 0.3635 - val_accuracy: 0.8724\n",
            "Epoch 131/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3597 - accuracy: 0.8693 - val_loss: 0.4083 - val_accuracy: 0.8642\n",
            "Epoch 132/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3638 - accuracy: 0.8684 - val_loss: 0.3850 - val_accuracy: 0.8791\n",
            "Epoch 133/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3567 - accuracy: 0.8711 - val_loss: 0.3541 - val_accuracy: 0.8748\n",
            "Epoch 134/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3596 - accuracy: 0.8692 - val_loss: 0.4216 - val_accuracy: 0.8591\n",
            "Epoch 135/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3528 - accuracy: 0.8721 - val_loss: 0.3596 - val_accuracy: 0.8754\n",
            "Epoch 136/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3550 - accuracy: 0.8699 - val_loss: 0.5116 - val_accuracy: 0.8444\n",
            "Epoch 137/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3642 - accuracy: 0.8656 - val_loss: 0.4229 - val_accuracy: 0.8604\n",
            "Epoch 138/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3525 - accuracy: 0.8717 - val_loss: 0.3653 - val_accuracy: 0.8728\n",
            "Epoch 139/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3653 - accuracy: 0.8667 - val_loss: 0.3645 - val_accuracy: 0.8734\n",
            "Epoch 140/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3545 - accuracy: 0.8720 - val_loss: 0.3866 - val_accuracy: 0.8688\n",
            "Epoch 141/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3670 - accuracy: 0.8681 - val_loss: 0.4054 - val_accuracy: 0.8619\n",
            "Epoch 142/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3524 - accuracy: 0.8705 - val_loss: 0.3499 - val_accuracy: 0.8734\n",
            "Epoch 143/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3564 - accuracy: 0.8710 - val_loss: 0.4453 - val_accuracy: 0.8569\n",
            "Epoch 144/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3536 - accuracy: 0.8735 - val_loss: 0.3807 - val_accuracy: 0.8647\n",
            "Epoch 145/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3592 - accuracy: 0.8710 - val_loss: 0.4134 - val_accuracy: 0.8650\n",
            "Epoch 146/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3484 - accuracy: 0.8724 - val_loss: 0.3785 - val_accuracy: 0.8694\n",
            "Epoch 147/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3552 - accuracy: 0.8703 - val_loss: 0.4687 - val_accuracy: 0.8428\n",
            "Epoch 148/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3527 - accuracy: 0.8702 - val_loss: 0.4129 - val_accuracy: 0.8670\n",
            "Epoch 149/150\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.3540 - accuracy: 0.8702 - val_loss: 0.3964 - val_accuracy: 0.8684\n",
            "Epoch 150/150\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3464 - accuracy: 0.8743 - val_loss: 0.3623 - val_accuracy: 0.8772\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f83528bfd50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCsPdpfymAIw"
      },
      "source": [
        "To learn more about the data augmentations available, see [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\r\n",
        "\r\n",
        "Note that this model is far from optimised, we don't get close to 100% training accuracy so you could definitely improve on this. But the data augmentation has led to us not overfitting.\r\n"
      ]
    }
  ]
}
